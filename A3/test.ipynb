{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/a3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/martincarrasco/Documents/VU/Y1P4/ANLP/AdvancedNLP-VU/A3/utils.py:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"seqeval\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/a3/lib/python3.10/site-packages/datasets/load.py:756: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "import datasets\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset, Sequence, ClassLabel, Features, Value\n",
    "import evaluate\n",
    "from preprocessing import preprocessing\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "import numpy as np\n",
    "import random\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing\n",
    "First, we will define some paths and constant variables.\n",
    "+ `DATA_NAME`: The name of the file to use for training\n",
    "+ `MODE`: It can either be `train` or `test` depending on wether a model already exists or not\n",
    "+ `PER_DS`: It is the percentage of the dataset that is sampled for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILE_NAME = 'en_ewt-up-dev.conllu'\n",
    "TEST_FILE_NAME = 'en_ewt-up-test.conllu'\n",
    "DEV_FILE_NAME = 'en_ewt-up-dev.conllu'\n",
    "MODE='dev'\n",
    "PER_DS = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Calling external libraries\n",
    "We will call the preprocessing library with the `DATA_NAME` file that we used earlier. It should be located in the `data/raw` path for it to be recongnized. It will go over the the CONLL-U file and create a dataframe with it as well as organize the argument label and repeat the tokes as many times at there are predictes in a given sentence. Consequently, if there are 3 predicates in a sentence, there will be 3 rows with the same token per sentence with a different target label depending on if they are part of the argument or not and which argument they are for that predicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = TRAIN_FILE_NAME if MODE == 'train' else TEST_FILE_NAME if MODE == 'test' else DEV_FILE_NAME\n",
    "df = preprocessing(filename)\n",
    "label_list = list(df['label'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Agregation per sentence\n",
    "We will agregate the dataframe based on `sentence_id`. This means that each row will now represent **one sentence**. Then, each row will contain a list of tokens for which corresponding lists of lemmas, predicates and labels are assigned. We perform this aggregation to facilitate then traning procedure, since we will to sequence to sequence tagging we want to pass a whole sequence and receive the output for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df = df.groupby(['sentence_id']).agg(lambda x: x.tolist()).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3  Dataset construction\n",
    "The `huggingface` set of libraries provides a very good wrapper for processing datasets. For that reason, we will transform our data to that format by giving the types of the `features` and then passing our raw data to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = Features({\n",
    "    'token_id': Sequence(feature=Value('float32')),\n",
    "    'sentence_num': Sequence(feature=Value('int32')),\n",
    "    'token': Sequence(feature=Value('string')),\n",
    "    'lemma': Sequence(feature=Value('string')),\n",
    "    'upos': Sequence(feature=Value('string')),\n",
    "    'POS': Sequence(feature=Value('string')),\n",
    "    'feats': Sequence(feature=Value('string')),\n",
    "    'head': Sequence(feature=Value('string')),\n",
    "    'deprel': Sequence(feature=Value('string')),\n",
    "    'deps': Sequence(feature=Value('string')),\n",
    "    'misc': Sequence(feature=Value('string')),\n",
    "    'predicate': Sequence(feature=Value('string')),\n",
    "    'predicate_token': Sequence(feature=Value('string')),\n",
    "    'predicate_token_id': Sequence(feature=Value('int32')),\n",
    "    'sentence_id': Value('int32'),\n",
    "    'label': Sequence(feature=ClassLabel(names=label_list)),\n",
    "\n",
    "})\n",
    "\n",
    "ds = Dataset.from_pandas(sent_df[list(features.keys())], features=features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 Dataset filtering\n",
    "Here we sample $k$ observations uniformly from the dataset, where $k=N*\\alpha$ and $\\alpha$ is `PER_DS` or the porcentage of the dataset we want to use for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "248"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ds.select(random.sample(range(len(ds)), int(len(ds)*PER_DS)))\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5 Tokenization \n",
    "To perform tokenization we use the BERT base tokenizer from the `bert-base-uncased` model, so that is the standard BERT implementation without separate tokens for word cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "SEP_TOKEN_ID = tokenizer.all_special_ids[tokenizer.all_special_tokens.index('[SEP]')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5.1 Actual tokenization and alignment\n",
    "Here we call a function that will return a dataset in terms of `input_ids` and `attention_masks`. It will construct the proposed input as **CITE**, where we have `[CLS] sent [SEP] pred [SEP]`, giving the model the whole sentence and then the predicate at the end of the sentence. It will also construt the corresponding true labels on this, assigning the tokens to the true labels. Words we do not want to predict a label for are marked with an integer that is generally $-100$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import tokenize_and_align_labels\n",
    "ex_sent_row = ds[0]\n",
    "predicate = ex_sent_row['predicate_token'][0]\n",
    "ex_sent = ex_sent_row['token']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to inspect how does our tokenization and alignment works. First, we check how the `predicate` is tokenized. First we take a look at the `predicate`. We pick the first one because according to our implementation it will contained a repeated list of predicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'were'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to see how its tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2020, 102], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_predicate = tokenizer([predicate], is_split_into_words=True)\n",
    "tok_predicate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Event though our predicate is only one word, the tokenization is returning different ids. Lets look closer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'were', '[SEP]']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tok_predicate['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see know that it has been encoded as 2 *special tokens* plus the actual word has been divided into 3 sub-tokens. Let's look  at what happens if we construct our input sentence by appending the `predicate` to the `sentence`. Also notice we will apply `truncation` and `padding`. Truncation will remove tokens after token $64$ and `padding` will ensure all the resulting tokenized sentences have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_whole = tokenizer(ex_sent, [predicate], padding='max_length', max_length=64, truncation=True, is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the amount of tokens after *tokenization* has grown. (also because of the PAD token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 9\n"
     ]
    }
   ],
   "source": [
    "print(len(tok_whole['input_ids']), len(ex_sent) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the sentence turned back into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'four',\n",
       " 'months',\n",
       " 'later',\n",
       " ',',\n",
       " 'we',\n",
       " 'were',\n",
       " 'married',\n",
       " '.',\n",
       " '[SEP]',\n",
       " 'were',\n",
       " '[SEP]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tok_whole['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that `predicate` has been appended to the end corrently. Note that also `token_type_ids` will change and instead of containing just 0s will contain 1s for our next word. Notice the `[PAD]` token at the end.\n",
    "\n",
    "Next, we will see how the labeling changes, since the output is also a number, we need to map our outputs to the appropriate labels. `word_ids` will return a list which has an index for each token and the word it respresents. If it is a *subtoken* it will be repeated and if its a special token it will be `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tok_whole.word_ids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct our labels, we only care about labeling the sentence that comes first so we need to construct a way to identify the first set of tokens and discard the rest. For this, we can tokenize the sentence again and just set all the other tokens to a value ignored by PyTorch $(-100)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_sent_alone = tokenizer(ex_sent, is_split_into_words=True)\n",
    "label_ids = []\n",
    "\n",
    "for i, word_idx in enumerate(tok_whole.word_ids()):\n",
    "    if word_idx is None:\n",
    "        # If it is a special token do not add a label\n",
    "        label_ids.append(-100)\n",
    "    elif i >= len(tok_sent_alone['input_ids']):\n",
    "        # If the token is part of the predicate do not add a label\n",
    "        label_ids.append(-100)\n",
    "    else: \n",
    "        # Set the label of the first token of each word\n",
    "        label_ids.append(ex_sent_row['label'][word_idx])\n",
    "label_names = ds.features['label'].feature.names\n",
    "decoded_labels = [label_names[idx] if idx != -100 else 'O' for idx in label_ids]\n",
    "(len(decoded_labels), len(tok_whole['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now they have the same length and on a closer look we will see that it only contains labels for the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'V',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply a map to calculate that for every row in our `Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 248/248 [00:00<00:00, 1398.62 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = ds.map(lambda x: tokenize_and_align_labels(tokenizer, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can look at the  labels of the first row in the dataset already tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-100,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['labels'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Baseline model training\n",
    "Now we specify some general information about the model we are going to train and the hyper-parameters we will use.\n",
    "\n",
    "+ `LR`: Learning rate for the weights (amount of adjustment to the gradients on update of weights)\n",
    "+ `EPOCHS`: The full runs we do on the training data \n",
    "+ `WEIGHT_DECAY`: A normalization parameter applied to the weights each iteration\n",
    "+ `BATCH_SIZE`: The amount of batches where to sum of gradients before performing an update. It can be though of like the **step-size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'SRL'\n",
    "BATCH_SIZE = 32\n",
    "model_name = 'bert-base-uncased'\n",
    "LR =2e-5\n",
    "EPOCHS = 3\n",
    "WEIGHT_DECAY = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TrainerArgument` class will let of specify all this information as well as a model name to put our model on the Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(label_list))\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Model training\n",
    "Now we actually get to training. We remove irrelevant columns from the dataset and pass all of our information to the actual trainer. Then we run the training and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import compute_metrics\n",
    "td = tokenized_datasets.remove_columns(ds.column_names)\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=td,\n",
    "    eval_dataset=td,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=lambda x: compute_metrics(x, label_list),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 8/24 [00:08<00:14,  1.10it/s]\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 8/24 [00:10<00:14,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8812035322189331, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.8473338203067933, 'eval_runtime': 1.9766, 'eval_samples_per_second': 125.467, 'eval_steps_per_second': 4.047, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 16/24 [00:25<00:11,  1.38s/it]\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 16/24 [00:27<00:11,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8059567809104919, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.8473338203067933, 'eval_runtime': 2.2429, 'eval_samples_per_second': 110.57, 'eval_steps_per_second': 3.567, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [00:41<00:00,  1.92s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [00:43<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7894327640533447, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.8473338203067933, 'eval_runtime': 2.0201, 'eval_samples_per_second': 122.767, 'eval_steps_per_second': 3.96, 'epoch': 3.0}\n",
      "{'train_runtime': 43.4848, 'train_samples_per_second': 17.109, 'train_steps_per_second': 0.552, 'train_loss': 0.8926275571187338, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(\"bert_model_dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Base model inference\n",
    "In this step we load the test data from CONLL-U and test the labeling capabilities. We will test the labeling of arguments, their identification and the labeling of predicates as well (`V`). We will then plot these labelings and look at the scores.\n",
    "For now, we use the `classification_report` of Scikit-Learn since the `seqeval` method of HuggingFace is broker if you do not want to evaluate `NER`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 64\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "#import dotenv\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "\n",
    "\n",
    "#dotenv.load_dotenv(\".env\", override=True)\n",
    "\n",
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "trained_model = AutoModelForTokenClassification.from_pretrained(\"bert_model_dev\", local_files_only=True).to('cpu')\n",
    "\n",
    "label_list = list(ds.features['label'].feature.names)\n",
    "with torch.inference_mode():\n",
    "\n",
    "    tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "    input_ids = tokenized_datasets['input_ids'][0].expand(1, -1)\n",
    "    attention_mask = tokenized_datasets['attention_mask'][0].expand(1, -1)\n",
    "    logits = trained_model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "    predicted_token_class_ids = torch.argmax(logits, dim=2).cpu()\n",
    "\n",
    "    predicted_token_class = [label_list[t] for t in predicted_token_class_ids[0].numpy().tolist()]\n",
    "    y_true = tokenized_datasets['labels'][0][:len(predicted_token_class)]\n",
    "    y = predicted_token_class_ids[0].numpy().tolist()\n",
    "    cr = classification_report(y, y_true, labels=label_list, zero_division=0, output_dict=True)\n",
    "    df = pd.DataFrame.from_dict(cr)\n",
    "    df.to_csv('metrics_bert_base.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Advanced model\n",
    "Our advanced model is based on NegBERT **CITE**.  NegBERT uses an annotation schema where certain groups of words are anotated of the labels with the aim of incorporating more information about *negation cues* into the model. According to the authors, this would in turn allow for better cue detection and span detection. We are trying to achieve something similar in the baseline model by appending the predicate at the end, however we do not use a MLP as an additionaly head of the output of the model. However, we will add more information to the input adding the base form of the predicate after the baseline. Thus resulting in `[CLS] sent [SEP] pred pred_base [SEP]` where `pred_base` would be the lemma of the predicate and the classification by PropBank.\n",
    "\n",
    "To do this we only need to modify our existing utility function for creating the tokenized dataset and add some more steps to it. With that, we just change the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'four',\n",
       " 'months',\n",
       " 'later',\n",
       " ',',\n",
       " 'we',\n",
       " 'were',\n",
       " 'married',\n",
       " '.',\n",
       " '[SEP]',\n",
       " 'were',\n",
       " 'be',\n",
       " '.',\n",
       " '03',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = ds[0]\n",
    "\n",
    "tok_base = tokenizer(row['token'], is_split_into_words=True)\n",
    "tok_pred_word = tokenizer(row['predicate_token'], is_split_into_words=True)\n",
    "tok_pred_base = tokenizer(row['predicate'], is_split_into_words=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tok_in = tokenizer(row['token'], [row['predicate_token'][0], row['predicate'][0]], is_split_into_words=True)\n",
    "tokenizer.convert_ids_to_tokens(tok_in['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see this simple change already gives us the desire results. We proceed to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import tokenize_and_align_labels_2\n",
    "tokenized_datasets_2 = ds.map(lambda x: tokenize_and_align_labels_2(tokenizer, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(label_list))\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from utils import compute_metrics\n",
    "td_2 = tokenized_datasets_2.remove_columns(ds.column_names)\n",
    "trainer_2 = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=td_2,\n",
    "    eval_dataset=td_2,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=lambda x: compute_metrics(x, label_list),\n",
    ")\n",
    "\n",
    "trainer_2.train()\n",
    "trainer_2.save_model(\"bert_model_advanced\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anlpa1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
