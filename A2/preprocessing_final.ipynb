{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fda4edc-7f6a-464b-b17a-415435d4dcdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def adding_headers_and_add_IDs_to_sentences(input_filepath, output_filepath):\n",
    "    sentence_id = 1  # Initialize sentence ID counter\n",
    "\n",
    "    # Open input file for reading and output file for writing    \n",
    "    with open(input_filepath, 'r', encoding='utf-8') as infile, \\\n",
    "         open(output_filepath, 'w', encoding='utf-8') as outfile:\n",
    "        for line in infile:\n",
    "            line = line.strip()  # Remove leading/trailing whitespaces\n",
    "\n",
    "            # Skip lines that are either empty or start with '#'\n",
    "            if line and not line.startswith('#'):\n",
    "                # Write the line along with the sentence ID to the output file\n",
    "                outfile.write(f'{sentence_id}\\t{line}\\n')\n",
    "            else:\n",
    "                # If the line is empty, it marks the end of a sentence\n",
    "                if not line:\n",
    "                    sentence_id += 1  # Increment sentence ID for the next sentence\n",
    "\n",
    "                    \n",
    "    # Open the processed output file for reading         \n",
    "    with open(output_filepath, encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Split each line into columns based on the tab character\n",
    "    data = [line.strip().split('\\t') for line in lines]\n",
    "    \n",
    "    # Find the maximum number of columns in any row\n",
    "    max_columns = max(len(row) for row in data)\n",
    "    print(f'max number of columns: {max_columns}')\n",
    "    \n",
    "    # Create a DataFrame with column names 'col_0', 'col_1', ..., 'col_(max_columns-1)'    \n",
    "    new_df = pd.DataFrame(data, columns=[f'col_{i}' for i in range(max_columns)])\n",
    "\n",
    "# --- code above not changed ---\n",
    "\n",
    "    \n",
    "    # Rename only the first 11 columns (sentence_num + 10 other - see assignment guidelines)\n",
    "    additional_cols = [f'col_{i}' for i in range(12, max_columns)]    ### 11 changed to 12\n",
    "    new_df.columns = ['sentence_id', 'token_id', 'token', 'lemma', 'UPOS', 'POS', 'grammar', 'head_id', 'dependency_relation',\n",
    "                      'head_dependency_relation', 'additional_info', 'PropBank_frames'] + additional_cols   ### 'token_id' added, 'is_predicate' changed to 'PropBank_frames', 'sentence_num' changed to 'sentence_id' \n",
    "    \n",
    "    return new_df        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "    \n",
    "#     # Perform the preprocessing and save the result to a CSV file\n",
    "#     df = adding_headers_and_add_IDs_to_sentences('../Data/en_ewt-up-train.conllu', '../Data/en_ewt-up-train_new.conllu')\n",
    "#     # df.to_csv('../Data/train(Nur).tsv', index=False)\n",
    "#     df.to_csv('../Data/train_header_added.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8592bd1b-b018-46b8-bd2b-072a596b747f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max number of columns: 47\n",
      "max number of columns: 30\n"
     ]
    }
   ],
   "source": [
    "# paths to the original training data, new traing data, and expected training data saved as a tsv file\n",
    "train_data = '../data/en_ewt-up-train.conllu'\n",
    "train_data_new= '../data/train_senID_added.conllu'\n",
    "train_tsv = '../data/train_header_added.tsv'\n",
    "\n",
    "# paths to the original test data, new test data, and expected test data saved as a tsv file\n",
    "test_data = '../data/en_ewt-up-test.conllu'\n",
    "test_data_new = '../data/test_senID_added.conllu'\n",
    "test_tsv = '../data/test_header_added.tsv'\n",
    "\n",
    "# call the function for both the training and test datasets, then save the results to TSV files respectively\n",
    "train_df = adding_headers_and_add_IDs_to_sentences(train_data, train_data_new)\n",
    "train_df.to_csv(train_tsv, sep='\\t', index=False)\n",
    "\n",
    "test_df = adding_headers_and_add_IDs_to_sentences(test_data, test_data_new)\n",
    "test_df.to_csv(test_tsv, sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8f31e2c-9b41-42d5-9c10-146db16567d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate_train_instances(input_path, output_path):\n",
    "    \n",
    "    with open(input_path, \"r\", encoding = \"utf-8\") as inputfile:\n",
    "        for line in inputfile:\n",
    "            columns = line.strip().split(\"\\t\")\n",
    "\n",
    "            # condition1: sentences with 1 predicate\n",
    "            if len(columns) == 13:\n",
    "                new_line = \"\\t\".join(columns)+\"\\n\"\n",
    "                with open(output_path, \"a\", encoding=\"utf-8\") as output:\n",
    "                    output.write(new_line)\n",
    "\n",
    "\n",
    "            # condition2: sentences with more than 1 predicates\n",
    "            else:\n",
    "                for i in range(14, 48):\n",
    "                    if len(columns) == i:\n",
    "                        # combine first 12 columns with different V&Arug column\n",
    "                        new_line = \"\\t\".join(columns[0:12] + [columns[i-1]]) + \"\\n\"\n",
    "                        with open(output_path, \"a\", encoding=\"utf-8\") as output:\n",
    "                            output.write(new_line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d938ee2-c120-4b9f-bc76-4b63666db205",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "duplicated_train = '../data/duplicated_train.tsv'\n",
    "\n",
    "duplicate_train_instances(train_tsv, duplicated_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e91a42de-9253-4464-aa97-158e329aeb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate_test_instances(input_path, output_path):\n",
    "    \n",
    "    with open(input_path, \"r\", encoding = \"utf-8\") as inputfile:\n",
    "        for line in inputfile:\n",
    "            columns = line.strip().split(\"\\t\")\n",
    "\n",
    "            # condition1: sentences with 1 predicate\n",
    "            if len(columns) == 13:\n",
    "                new_line = \"\\t\".join(columns)+\"\\n\"\n",
    "                with open(output_path, \"a\", encoding=\"utf-8\") as output:\n",
    "                    output.write(new_line)\n",
    "\n",
    "\n",
    "            # condition2: sentences with more than 1 predicates\n",
    "            else:\n",
    "                for i in range(14, 31):\n",
    "                    if len(columns) == i:\n",
    "                        # combine first 12 columns with different V&Arug column\n",
    "                        new_line = \"\\t\".join(columns[0:12] + [columns[i-1]]) + \"\\n\"\n",
    "                        with open(output_path, \"a\", encoding=\"utf-8\") as output:\n",
    "                            output.write(new_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbb046c5-caa5-47ac-9018-e465bda17f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_test = '../data/duplicated_test.tsv'\n",
    "\n",
    "duplicate_test_instances(test_tsv, duplicated_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b0e3f68-2622-4707-ab15-e98c76765515",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def add_true_lables_4arguments(input_file, output_file):\n",
    "    '''\n",
    "    This function add label to each token indicating if it is an argument.\n",
    "\n",
    "    Input: \n",
    "    -input_file: filepath to the input file\n",
    "    -output_file: filepath to the output file\n",
    "    '''\n",
    "    with open(input_file, 'r', newline='', encoding='utf-8') as infile, \\\n",
    "            open(output_file, 'w', newline='', encoding='utf-8') as outfile:\n",
    "\n",
    "        reader = csv.reader(infile, delimiter='\\t')\n",
    "        writer = csv.writer(outfile, delimiter='\\t')\n",
    "\n",
    "        for row in reader:\n",
    "            # check if the row is not empty\n",
    "            if len(row) == 13:\n",
    "                \n",
    "                # condition1：the current token is not an argument\n",
    "                if row[12] == \"_\" or row[12] == \"V\":\n",
    "                    row.append(\"O\")\n",
    "                    \n",
    "                # condition2：the current token is an argument \n",
    "                else:\n",
    "                    row.append(row[12])\n",
    "                                                                                        \n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f008c9f-ca8e-4d52-976b-106ea4b51cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths to save preprocessed training ans test data\n",
    "preprocessed_train = '../data/preprocessed_train.tsv'\n",
    "preprocessed_test = '../data/preprocessed_test.tsv'\n",
    "\n",
    "# call function to get preprocessed training and test datasets\n",
    "add_true_lables_4arguments(duplicated_train, preprocessed_train)\n",
    "add_true_lables_4arguments(duplicated_test, preprocessed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a04af1ec-8461-4c17-8201-8be1f5ab6170",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def change_tsv_header(input_file_path, new_header, output_file_path):\n",
    "    # Read the TSV file and store the data\n",
    "    with open(input_file_path, 'r', newline='', encoding='utf-8') as input_file:\n",
    "        reader = csv.reader(input_file, delimiter='\\t')\n",
    "        data = list(reader)\n",
    "\n",
    "    # Change the header\n",
    "    data[0] = new_header\n",
    "\n",
    "    # Write to a new file with the updated header\n",
    "    with open(output_file_path, 'w', newline='', encoding='utf-8') as output_file:\n",
    "        writer = csv.writer(output_file, delimiter='\\t')\n",
    "        writer.writerows(data)\n",
    "\n",
    "    print(f\"Header changed. New Header: {new_header}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a5b3b17-148d-441b-8900-d3e964832eaf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header changed. New Header: ['sentence_id', 'token_id', 'token', 'lemma', 'UPOS', 'POS', 'grammar', 'head_id', 'dependency_relation', 'head_dependency_relation', 'additional_info', 'PropBank_frames', 'annotation', 'label']\n",
      "Header changed. New Header: ['sentence_id', 'token_id', 'token', 'lemma', 'UPOS', 'POS', 'grammar', 'head_id', 'dependency_relation', 'head_dependency_relation', 'additional_info', 'PropBank_frames', 'annotation', 'label']\n"
     ]
    }
   ],
   "source": [
    "preprocessed_train_with_header = '../data/preprocessed_train_with_header.tsv'\n",
    "preprocessed_test_with_header = '../data/preprocessed_test_with_header.tsv'\n",
    "\n",
    "new_header = ['sentence_id', 'token_id', 'token', 'lemma', 'UPOS', 'POS', 'grammar', 'head_id', 'dependency_relation', 'head_dependency_relation', 'additional_info', 'PropBank_frames', 'annotation', 'label']\n",
    "\n",
    "change_tsv_header(preprocessed_train, new_header, preprocessed_train_with_header)\n",
    "change_tsv_header(preprocessed_test, new_header,preprocessed_test_with_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa00ef62-52b0-4fbf-b196-04dbde092914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410c08fb-5084-4064-b493-bd442630aee0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
